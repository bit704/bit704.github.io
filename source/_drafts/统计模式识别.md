---
title: 统计模式识别
categories: [Essay]
tags: [AI,Math]
---

统计模式识别梳理

<!--more-->

[toc]

## Parameter Estimation

假定概率密度的特定形式（比如高斯分布），只需要估计参数（比如均值、方差）即可。比如Bayes和ML。

Criterion for Classification Decision Rule：

- MAP (Maximum A Posteriori) Criterion

- ML (Maximum Likelihood) Criterion

- Bayes Criterion

- Other Criterion

假定总体$X$的PDF为$f(x\mid\theta)$，观测到一组样本$(X_1,X_2,...,X_n)=(x_1,x_2,...,x_n)$，需要估计参数$\theta$。

### Bayes

**贝叶斯估计**是贝叶斯学派观点。$\theta$是随机的，只能估计其分布。
$$
\pi (\boldsymbol \theta|\boldsymbol x)=\frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{m(\boldsymbol x)}=\frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{\int f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)d(\boldsymbol \theta)}
$$
$\pi (\boldsymbol \theta) $为参数$ \boldsymbol \theta$ 的先验分布（prior distribution），表示对参数$ \boldsymbol \theta $的主观认识，是非样本信息。$ \pi (\boldsymbol \theta|\boldsymbol x) $为参数$ \boldsymbol \theta $的后验分布（posterior distribution）。因此，贝叶斯估计可以看作是，在假定$ \boldsymbol \theta $服从 $\pi (\boldsymbol \theta)$ 的先验分布前提下，根据样本信息去校正先验分布，得到后验分布 $\pi (\boldsymbol \theta|\boldsymbol x)$ 。由于后验分布是一个条件分布，通常我们取后验分布的期望作为参数的估计值。

使用**贝叶斯风险**用来度量一个估计量的好坏。对于待估参数$\theta$和$x$，参数$\theta$的贝叶斯估计值为$\hat{\theta}$，那么贝叶斯风险为：
$$
\R=E_{\theta,x}[l(\theta,\hat{\theta})]
$$
$l$为评价风险的损失函数。

### MAP

可以看作Bayes的特殊情况。

贝叶斯估计中后验分布的计算往往是非常棘手的。**最大后验（MAP）估计**利用了无法从观测样本获得的来自先验的信息，是贝叶斯估计的一种近似解。使后验分布$\pi (\boldsymbol \theta|\boldsymbol x)$极大化的$\hat{\theta}_{map}$是$\theta$的MAP估计值。
$$
\\\boldsymbol {\hat \theta}_{map}=arg \underset {\boldsymbol \theta}{\max} \pi (\boldsymbol \theta|\boldsymbol x)=arg \underset {\boldsymbol \theta}{\max} \frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{m(\boldsymbol x)}=arg \underset {\boldsymbol \theta}{\max} {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}=arg \underset {\boldsymbol \theta}{\max} (\log {f(\boldsymbol x|\boldsymbol \theta)+ \log \pi (\boldsymbol \theta))}\\
$$
如果将机器学习结构风险中的正则化项对应为上式的$\log \pi (\boldsymbol \theta)$，那么带有这种正则化项的ML学习就可以被解释为MAP。

### ML

可以看作MAP的特殊情况。

**极大似然（ML）估计**是频率学派观点。$\theta$客观存在，只是未知。

使样本$(X_1,X_2,...,X_n)=(x_1,x_2,...,x_n)$更容易观测到（发生概率更大）的$\hat{\theta}_{ml}$是$\theta$的ML估计值。
$$
L(\boldsymbol \theta|\boldsymbol x)=f(\boldsymbol x|\boldsymbol \theta)=f(x_1,x_2,\dots,x_n|\boldsymbol \theta)=\prod^{n}_{i=1}f(x_i|\boldsymbol \theta) \\ \boldsymbol {\hat \theta}_{mle}=arg\underset {\boldsymbol \theta}{\max} L(\boldsymbol \theta|\boldsymbol x)
$$

### eg.

拿到一枚未知的硬币，估计其正面出现概率$\theta$。

观测到样本$X$：反正正正正反正正正反

似然函数：$f(X\mid\theta)=(1-\theta)\times\theta\times\theta\times\theta\times\theta\times(1-\theta)\times\theta\times\theta\times\theta\times(1-\theta)=\theta^7(1-\theta)^3$

求$f(X\mid\theta)$图像和使其最大的$\theta$值：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,1,100)
y = x**7*(1-x)**3

plt.plot(x,y)
plt.show()

print(x[y.argmax()])
```

>  ![](https://cdn.jsdelivr.net/gh/bit704/blog-image-bed@main/image/2022-09-30-ML估计值.png)
>
> 0.696969696969697

得出使其最大的$\theta$大约是0.7。此即ML估计。

引入$\theta$的先验分布为高斯分布，即$f(\theta)$为均值0.5，方差0.1的高斯函数。
$$
f(\theta)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\theta-\mu)^2}{2\sigma^2}},\mu=0.5,\sigma=0.1
$$
求$f(X\mid\theta)f(\theta)$图像和使其最大的$\theta$值：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,1,100)
y = x**7*(1-x)**3

sigma = 0.1
mu = 0.5
gauss = 1/(sigma * np.sqrt(2*np.pi)) * np.power(np.e,-((x-mu)**2)/(2*sigma**2))
y *= gauss

plt.plot(x,y)
plt.show()

print(x[y.argmax()])
```

> ![](https://cdn.jsdelivr.net/gh/bit704/blog-image-bed@main/image/2022-09-30-MAP估计值.png)
>
> 0.5555555555555556

得出使其最大的$\theta$大约是0.56。此即MAP估计。

### Mixture models 





### EM







## Non-parametric Density Estimation

不假定关于概率密度的任何知识。比如核密度估计和最近邻规则。



## 参考资料

北京理工大学邸慧军老师   讲课内容

[极大似然估计和贝叶斯估计 - 知乎](https://zhuanlan.zhihu.com/p/61593112)

[机器学习准则（期望风险、经验风险、结构风险） - 知乎 ](https://zhuanlan.zhihu.com/p/159189617)

