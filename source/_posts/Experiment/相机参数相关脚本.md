---
title: 相机参数相关脚本
layout: post
categories: [Experiment]
mathjax: false
---

相机参数

<!-- more -->

部分结合我的[代码仓库]( https://github.com/bit704/visual-poses)

## 1.利用colmap的相机外参可视化

利用colmap，算出每张照片的相机外参的四元数和位移向量。

根据四元数求得旋转矩阵。（旋转矩阵每一个列向量都是单位向量，且两两正交）

将旋转矩阵(3\*3)与位移向量(3\*1)还有底部齐次向量（1\*4 [0,0,0,1]）**拼接**得到世界到相机矩阵w2c(4*4)。

世界到相机矩阵w2c**求逆**到得到相机到世界矩阵c2w。

从相机到世界矩阵c2w中**分割**出相机到世界的旋转矩阵和位移向量。

把上述求出的旋转矩阵，与世界坐标系xyz轴的单位向量（列向量）分别相乘，即得到相机坐标系的xyz轴的单位向量。上述求出的位移向量，以世界坐标系的原点为起点，终点即为相机坐标系的原点。

如此便可绘出每张照片对应的相机坐标系的**原点与xyz轴**。

<font color="red">注意：</font>

绘制之后可以发现，colmap的世界坐标系里，各个相机视角的中心轴并不垂直于xy平面。

NeRF对于相机参数的预处理中没有解决这个问题，使用的世界坐标系各个相机视角的中心轴仍然不垂直于xy平面，但是渲染演示视频时使用的一圈新视角中心轴垂直于xy平面。因此使用环视数据集时，会出现演示视频的相机移动轨迹与采样的环视轨迹不一致。

instant-ngp对于相机参数的预处理解决了这个问题。

## 2.LLFF、NeRF对于相机参数的处理

结合我写的[NeRF全流程代码解析](NeRF全流程代码解析.md)

**LLFF**也是利用colmap，算出每张照片的相机外参的四元数和位移向量。

求出相机到世界矩阵c2w的步骤**与第一部分相同**。

将c2w(4\*4)去掉底部齐次向量(1\*4)，再与hwf向量(3*1，即宽高焦距，相机内参)拼接得到单个pose矩阵(3\*5)。

poses矩阵的形状为(3\*5\*num)，num是照片的数量。

对poses矩阵的第二个轴（列）进行重排。[1,2,3,4,5]的列顺序**重排**为[2,1,-3,4,5]，实际上就是旋转了一下坐标系，左手系还是右手系不变。

poses数组最后被拉平拼接保存到poses_bounds.npy中。

**NeRF**从poses_bounds.npy中加载得到poses矩阵。

将poses矩阵中的第四列（即hwf）**根据实际输入图像的高宽进行修改**。例如输入图像为原图1/4大小的图像，高宽均缩小为原图的1/4，焦距缩小为1/4。

对poses矩阵的第二个轴（列）进行重排。[1,2,3,4,5]的列顺序重排为[2,-1,3,4,5]，也是旋转了一下坐标系，左手系还是右手系不变。**这并不是逆转了LLFF做的重排。**因此，在对NeRF读取得poses进行可视化的时候，需要对z轴和y轴做反向，来逆转LLFF、NeRF对colmap求出的poses的重排造成的坐标系旋转。对instant-ngp也是一样的。

## 3.新视角生成

见 [新视角生成](https://github.com/bit704/visual-poses#%E4%BA%8C%E7%94%9F%E6%88%90%E6%96%B0%E8%A7%86%E8%A7%92)



















