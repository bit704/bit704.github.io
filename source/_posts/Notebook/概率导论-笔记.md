---
title: 《概率导论（第2版）》笔记
cover: https://tva1.sinaimg.cn/large/0077Un8Egy1h5ek22yojij30zk0hs40j.jpg
banner: https://tva1.sinaimg.cn/large/0077Un8Egy1h5ek22yojij30zk0hs40j.jpg
layout: post
categories: [Notebook]
tags: [Math,Pobability]
mathjax: true
---

原书作者  *Dimitri P. Bertsekas*      *John N. Tsitsiklis*

<!-- more -->

Introduction to Probability 

译者  郑忠国    童行伟

人民邮电出版社



## 第1章 样本空间与概率

### 1.1集合

概率论大量应用集合运算，引入集合相关的记号和术语。

集合 $\{ x\in \Omega\mid x\notin S \}$ 称为集合$S$相对于$\Omega$的补集，记作$S^C$。 $\Omega^C=\emptyset$ 。

**德摩根定律**：$(\bigcup_{n} S_{n} )^c=\bigcap_{n}S_{n}^c$   $(\bigcap_{n} S_{n} )^c=\bigcup_{n}S_{n}^c$

### 1.2 概率模型

概率模型是对不确定现象的数学描述，每个概率模型都关联一个试验。

**概率模型**基本构成：

- 样本空间$\Omega$，这是一个试验的所有可能结果的集合。
- 概率律，概率律为试验结果的集合$A$（称之为**事件**，即样本空间的子集）确定一个非负数$P(A)$（称之为事件A的**概率**）。而这个非负数刻画了我们对事件$A$的认识或所产生的信念的程度。

许多实验本身具有序贯的特征，常用**序贯树形图**刻画样本空间中的试验结果。

**概率公理**：

1. 非负性。对一切事件$A$，满足$P(A)\ge0$。

2. 可加性。设$A$和$B$为两个互补相交的集合（概率论中称为互不相容的事件），则它们的并满足：$P(A\bigcup B)=P(A)+P(B)$。可以向一般情形推广。

3. 归一性。整个样本空间$\Omega$（称为必然事件）的概率为1，即$P(\Omega)=1$。

**离散概率律**：

设样本空间由有限个可能的结果组成，则事件的概率可由组成这个事件的试验结果的概率所决定。
$$
P(\{s_1,s_2,...,s_n\})=P(s_1)+P(s_2)+...+P(s_n)
$$


### 1.3 条件概率

给定$B$发生之下事件$A$的条件概率，记作$P(A\mid B)$。

$P(A\mid B)=\frac{P(A\bigcap B)}{P(B)} $。

**乘法规则**：
$$
P( { \bigcap_{i=1}^{n}}A_i)=P(A_1)P(A_2\mid A_1)P(A_3\mid A_1\bigcap A_2)...P(A_n\mid {\bigcap^{n-1}_{i=1}}A_i)
$$

### 1.4 全概率定理和贝叶斯准则

设$A_1,A_2,...,A_n$是一组互不相容的事件，它形成样本空间的一个分割（每一个试验结果必使其中一个事件发生）。又假定对每一个$i$，$P(A_i)>0$。

**全概率定理**：

则对于任意事件$B$，下列公式成立：$P(B)=P(A_1\bigcap B)+...+P(A_i\bigcap B)$

**贝叶斯准则**：

则对于任意满足$P(B)>0$的事件$B$，下列公式成立：$P(A_i\mid B)=\frac{P(A_i)P(B\mid A_i)}{P(B)}$

若$P(A_i\mid B)$代表新得到信息$B$之后$A_i$出现的概率，称之为**后验概率**，原来的$P(A_i)$称为**先验概率**。

### 1.5 独立性

若$P(A\mid B)=P(A)$，称事件$A$是独立于事件$B$的。上式等价于$P(A\bigcap B)=P(A)P(B)$。

若$P(A\bigcap B\mid C)=P(A\mid C)P(B\mid C)$，则称$A$和$B$在给定$C$之下条件独立。

### 1.6 计数法

$n$个对象的排列数：$n!$

$n$个对象中取$k$个对象的排列数：$\frac{n!}{(n-k)!}$

$n$个对象中取$k$个对象的组合数：$\begin{pmatrix} n\\k \end{pmatrix}=\frac{n!}{k!(n-k)!}$

将$n$个对象分成$r$个组的分割数，其中第$i$个组具有$n_i$个对象：$\begin{pmatrix} n\\n_1,n_2,...,n_r \end{pmatrix}=\frac{n!}{n_1!n_2!...n_r!}$

## 第2章 离散随机变量

### 2.1 基本概念

在一个试验的概率模型之下：

- **随机变量**是试验结果的实值函数
- 随机变量的函数定义了另一个随机变量
- 对于一个随机变量，我们可以定义一些平均量，例如均值和方差
- 可以在某事件或某随机变量的条件之下定义一个随机变量
- 存在一个随机变量与某事件或某随机变量项目独立的概念

离散随机变量的取值范围智能是有限多个值或可数无限多个值。

一个离散随机变量有一个**分布列**，它对于随机变量的每一个取值给出一个概率。

离散随机变量的函数也是一个离散随机变量。

### 2.2 分布列

$p_X(x)=P(\{X=x\})$，**大写字母表示随机变量，小写字母表示实数**。

$\sum_{x}p_X(x)=1$

**伯努利随机变量**：

抛掷一枚硬币，正面出现概率为$p$。$X$取值为$k$，$k$正面为1，反面为0。

$$
p_X(k)=\left\{ \begin{aligned}  p,k=1\\1-p,k=0  \end{aligned} \right.
$$
**二项随机变量**：

将硬币抛掷$n$次。$X$取值为$k$，$k$为得到正面的次数。
$$
p_X(k)=\begin{pmatrix} n\\\\k \end{pmatrix}p^k(1-p)^{n-k},k=0,1,...,n.
$$
**几何随机变量**：

$X$取值为$k$，$k$为连续抛掷硬币直到第一次出现正面所需的抛掷次数。
$$
p_X(k)=(1-p)^{k-1}p,k=1,2,...
$$
**泊松随机变量**：

$X$的分布列如下：
$$
p_X(k)=e^{-\lambda}\frac{\lambda^k}{k!},k=0,1,2,...
$$
当二项随机变量$n$很大、$p$很小时，$\lambda=np$，泊松随机变量的分布列是二项随机变量分布列很好的逼近：
$$
e^{-\lambda}\frac{\lambda^k}{k!} \approx \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k},k=0,1,...,n.
$$

### 2.3 随机变量的函数

随机变量的函数也是随机变量。

### 2.4 期望、均值和方差

**期望**：

设随机变量$X$的分布列为$p_X$。$X$的期望值由下式给出：
$$
E[X]=\sum_{x} xp_X(x)
$$
对于$X$的函数$g(X)$:
$$
E[g(X)]=\sum_x g(x)p_X(x)
$$
**方差**：

随机变量$X$的方差为：
$$
var(X)=E[(X-E[X])^2]
$$
其平方根记为标准差$\sigma_X$

**随机变量的线性函数的均值和方差**：

若$Y=aX+b$,则：
$$
E[Y]=aE[X]+b,  var(Y)=a^2var(X)
$$
**用矩表达的方差公式**：
$$
var(X)=E[X^2]-(E[X])^2
$$

### 2.5 多个随机变量的联合分布列

$$
p_{X,Y}(x,y)=P(X=x,Y=y)
$$

可以通过联合分布列求单个随机变量的分布列：
$$
p_X(x)= P(X=x) =\sum_y P(X=x,Y=y) = \sum_y P_{X,Y}(x,y)
$$
以上也称为边缘分布列。

### 2.6 条件

在某个事件$A(P(A)>0)$发生的条件下，随机变量$X$的**条件分布列**由下式定义：
$$
p_{X\mid A}(x)=P(X=x\mid A)=\frac{P(\{X=x\}\bigcap A)}{P(A)}
$$

### 2.7 独立性

设在某一试验中，$A$是一个事件，满足条件$P(A)>0$，又设$X$和$Y$是在同一个试验中的两个随机变量。

若对一切$x$，$p_{X\mid A}(x)=p_X(x)$，则称$X$相对于事件$A$独立。

若对一切$x$、$y$，$p_{X,Y}(x,y)=p_X(x)p_Y(y)$，则称$\{X=x\}$ 和$\{Y=y\}$相互独立。此时有$E[XY]=E[X]E[Y]$、$var(X+Y)=var(X)+var(Y)$。

## 第3章 一般随机变量

### 3.1 连续随机变量和概率密度函数

对于随机变量$X$，若存在一个非负函数$f_X$，使得
$$
P(X\in B)=\int_B f_X(x)dx
$$
对每一个实数轴上的集合$B$都成立，则称$X$为连续的随机变量，函数$f_X$就称为$X$的概率密度函数(PDF)。
$$
E[X]=\int_{-\infty}^{\infty}xf_X(x)dx
$$

### 3.2 分布函数

之前分别用分布列（离散情况）和概率密度函数（连续情况）来刻画随机变量$X$的取值规律。累计分布函数（CDF）可以同时刻画以上两者：
$$
若X离散，F_X(x)=P(X\le x)=\sum_{k\le x}p_X(k) \\
若X连续，F_X(x)=\int_{-\infty}^x f_X(t)dt
$$

### 3.3 正态随机变量

正态/高斯连续随机变量$X$的密度函数如下：
$$
f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/(2\sigma^2)}，E[X]=\mu,var(X)=\sigma^2
$$
线性变化之下随机变量的正态性保持不变。

期望为1方差为1的正态随机变量称为标准正态随机变量。

### 3.4 多个随机变量的联合概率密度

$$
P((X,Y)\in B)=\int\int_{(x,y)\in B} f_{X,Y}(x,y)dxdy
$$

### 3.5 条件

对于给定的事件$A(P(A)>0)$，$B$是实数轴上的任意集合，连续随机变量$X$的条件概率密度$f_{X\mid A}$是满足下列条件的函数：
$$
P(X\in B\mid A)=\int_B f_{X\mid A}(x)dx
$$
设$X$和$Y$为联合连续的随机变量，其联合、边缘、条件概率密度函数是相互关联的：
$$
f_{X,Y}(x,y)=f_Y(y)f_{X\mid Y}(x\mid y)
$$

### 3.6 连续贝叶斯准则

$$
f_{X\mid Y}(x\mid y)=\frac{f_X(x)f_{Y\mid X}(y\mid x)}{f_Y(y)}=\frac{f_X(x)f_{Y\mid X}(y\mid x)}{\int_{-\infty}^{\infty}f_X(t)f_(Y\mid X)(y\mid t)dt}
$$

## 第4章 随机变量的深入内容

### 4.1 随机变量函数的分布密度函数

连续随机变量$X$的函数$Y=g(X)$的分布密度函数（CDF）：
$$
F_Y(y)=P(g(x)\le y)=\int_{\{x\mid g(x)\le y\}}f_X(x)dx
$$

PDF可由求导得到：
$$
f_Y(y)=\frac{dF_Y}{dy}(y)
$$

### 4.2 协方差和相关

$X$和$Y$的协方差记为$cov(X,Y)$，当其为0时$X$和$Y$不相关：
$$
cov(X,Y)=E[(X-E[X])(Y-E[Y])]
$$

$$
cov(X,Y)=E[XY]-E[X]E[Y]
$$

如果$X$和$Y$相互独立，则它们是不相关的，但逆命题不成立。

**相关系数**定义为：
$$
\rho(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}},\rho(X,Y)\in[-1,1]
$$
随机变量和的方差：
$$
var(X_1+X_2)=var(X_1)+var(X_2)+2cov(X_1,X_2)
$$

$$
var(\sum_{i=1}^{n}X_i)=\sum_{i=1}^{n}var(X_i)+\sum_{\{(i,j)\mid i\ne j\}}cov(X_i,Y_j)
$$

### 4.3 再论条件期望和条件方差

**重期望法则**：
$$
E[E[X\mid Y]]=E[X]
$$
**全方差法则**：
$$
var(X)=E[var(X\mid Y)]+var(E[X\mid Y])
$$

### 4.4 矩母函数

一个与随机变量$X$相关的矩母函数是一个参数$s$的函数$M_X(s)$，定义如下：
$$
M_X(s)=E[e^{sX}]
$$
数学期望和方差可以纳入到一个更一般的概念范畴中，那就是随机变量的矩。

设$X$为随机变量，$k$为正整数，如果$E(X^k)$存在，则称$E(X^k)$为$X$的$k$阶原点矩。如果$E((X-E(X))^k)$存在，则称$E((X-E(X))^k)$为$X$的$k$阶中心矩。

显然，一阶原点矩就是数学期望，二阶中心矩就是方差。

使用矩母函数可以方便地计算矩。

### 4.5 随机数个相互独立的随机变量之和

 记$X_1,X_2,...$为均值$\mu$、方差$\sigma^2$的同分布随机变量。记$N$为取值于正整数的随机变量。我们假定上述所有变量相互独立，下面考虑变量和
$$
Y=X_1+...+X_N
$$
那么：

- $E[Y]=E[X]E[N]$
- $var(Y)=var(X)E[N]+(E[X])^2var(N)$
- 矩母函数$M_Y(s)$可由计算矩母函数$M_N(s)$的公式得到，将其中的$e^S$全部替换成$M_X(s)$即可

## 第5章 极限理论

### 5.1 马尔可夫和切比雪夫不等式

**马尔克夫不等式**：

设随机变量$X$只取非负值，则对任意$a>0$，
$$
P(X\ge a)\le\frac{E[X]}{a}
$$
如果一个非负随机变量均值很小，它取大值的概率也很小。

**切比雪夫不等式**：

设随机变量$X$的均值为$\mu$，方差为$\sigma^2$，则对任意$c>0$，
$$
P(\mid X-\mu \mid\ge c)\le \frac{\sigma^2}{c^2}
$$
如果一个随机变量的方差很小，它取远离均值$\mu$的概率也很小。

### 5.2 弱大数定律

设$X_1,...,X_n,...$独立同分布，其公共分布的均值为$\mu$，则对任意的$\epsilon>0$，当$n\longrightarrow \infty $时，
$$
P(\mid M_n-\mu \ge \epsilon \mid)=P(\mid \frac{X_1+...+X_n}{n} - \mu \mid\ge\epsilon)\longrightarrow 0
$$
利用切比雪夫不等式可得：
$$
P(\mid M_n-\mu\mid\ge\epsilon)\le\frac{\sigma^2}{n\epsilon^2},\forall \epsilon>0
$$
n越大，$M_n$均值越接近公共分布均值，收敛于$\mu$。

### 5.3 依概率收敛

依概率收敛和数列收敛不同。

设$Y_1,Y_2,...$是随机变量序列（不必相互独立），对任意的$\epsilon>0和\delta>0$，存在$n_0$和实数a，使得对所有的$n\ge n_0$，都有
$$
P(\mid Y_n-a\mid \ge \epsilon)\le\delta
$$
$\epsilon$为精度，$\delta$为置信水平。

### 5.4 中心极限定理

设$X_1,X_2,...$是独立同分布的随机变量序列，序列的每一项的均值为$\mu$，方差为$\sigma^2$。记
$$
Z_n=\frac{X_1+...+X_n-n\mu}{\sqrt{n}\sigma}
$$
则$Z_n$的分布函数的极限分布为标准正态分布
$$
\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-z^2/2}dz
$$
即
$$
\lim_{n\to\infty} P(Z_n\le x)=\Phi(x),\forall x
$$
二项分布的棣莫弗-拉普拉斯近似

设$S_n$是服从参数为n和p的二项分布，n充分大，k和l是非负整数，则
$$
P(k\le S_n \le l)\approx \Phi(\frac{l+\frac{1}{2}-np}{\sqrt{np(1-p)}})-\Phi(\frac{k-\frac{1}{2}-np}{\sqrt{(np(1-p))}})
$$

### 5.5 强大数定律

设$X_1,X_2,...,X_n$是均值为$\mu$的独立同分布随机变量序列，则样本均值$M_n=(X_1+X_2+...+X_n)/n$以概率1收敛于$\mu$，即
$$
P(\lim_{n\to\infty}\frac{(X_1+X_2+...+X_n)}{n}=\mu)=1
$$
强大数定律的收敛是“以概率1收敛”，与若大数定律的收敛不同。弱大数定律是指$M_n$显著性偏离$\mu$的事件的概率$P(\mid M_n-\mu\mid\ge\epsilon)$，在$n\to\infty$时，趋于0。但是对任意有限的n，这个概率可以是正的。弱大数定律不能提供到底有多少会显著性偏离$\mu$，但是强大数定律可以。$M_n$以概率1收敛于$\mu$，这意味着对$\forall\epsilon>0$，偏离$\mid M_n-\mu\mid$超过$\epsilon$的，只能发生有限次。

## 第6章 伯努利和泊松过程

随机过程就是一串随机变量序列，但是更倾向于强调过程中产生的数据序列之间的相关关系。

### 6.1 伯努利过程

**与伯努利过程相关的随机变量**

服从参数n和p的二项分布。n次相继独立的试验成功的总次数$S$的分布。
$$
p_S(k)==\begin{pmatrix} n\\\\k \end{pmatrix}p^k(1-p)^{n-k},k=0,1,...,n.
$$

$$
E[S]=np,var(S)=np(1-p)
$$

服从参数为p的几何分布。相互独立重复的伯努利试验首次成功的总次数$T$的分布。
$$
p_T(t)=p(1-p)^{t-1},t=1,2 ...,
$$

$$
E[T]=\frac{1}{p},var(T)=\frac{1-p}{p^2}
$$

伯努利过程具有独立性和无记忆性。

### 6.2 泊松过程

泊松过程是伯努利过程的**连续**版本。两者都是到达过程。

**泊松过程相关的随机变量**

服从参数为$\lambda\tau$的泊松分布。泊松过程的强度为$\lambda$，在时间强度为$\tau$的区间内到达的总次数$N_\tau$的分布。
$$
p_{N_\tau}(k)=P(k,\tau)=e^{-\lambda\tau}\frac{(\lambda\tau)^k}{k!},k=0,1,...,
$$

$$
E[N_\tau]=var(N_\tau)=\lambda\tau
$$

服从参数为$\lambda$的指数分布。首次到达的时间$T$的分布。
$$
f_T(t)=\lambda e^{-\lambda t},t\ge 0,E[T]=\frac{1}{\lambda},var(T)=\frac{1}{\lambda^2}
$$

## 第7章 马尔可夫链

### 7.1 离散时间的马尔可夫链

用变量n表示时刻，在任意时刻n用$X_n$表示链的状态，所有可能状态组成有限集合$S$，称该集合为状态空间。

马尔可夫链由转移概率$p_{ij}$所描述：即当状态是i时，下一个状态等于j的概率是$p_{ij}$。
$$
p_{ij}=P(X_{n+1}=j\mid X_n=i),i,j\in S
$$
马尔可夫性质：下一个状态$X_{n+1}$的概率分布只依赖于前一个状态$X_n$。

转移概率一定是非负的，且其和为1。

马尔可夫链可以用转移概率矩阵或转移概率图刻画。

### 7.2 状态的分类

如果对于每个从$i$出发可达的状态$j$，相应地从$j$出发也可达$i$，则$i$是**常返**的。

如果$i$是常返态，那么从$i$可达的状态集合$A(i)$组成一个**常返类**。同一个常返类中所有状态都是相互可达的，但与常返类之外的状态不相互可达。

一个马尔可夫链的状态集合可以分解为一个或多个常返类，加上可能的一些非常返状态。

考虑一个常返类$R$

- 如果一类中的状态能被分成$d>1$个互不相交的子集$S_1,...,S_d$，满足所有的转移都是从子集$S_k$到$S_{k+1}$的（或到$S_1$，当$k=d$时）,则称该类为周期类。

- 一类$R$称为非周期的，当且仅当存在时刻n，使得对于任何$i,j\in R$，满足$r_{ij}(n)>0$。

### 7.3 稳态性质

考虑一个非周期的，单个常返类的马尔可夫链。那么，状态$j$和它对应的稳态概率$\pi_j$有如下性质。

(a) 对于每个$j$，我们有：
$$
\lim_{n\to\infty}r_{ij}(n)=\pi_j，对于所有的i
$$
(b) $\pi_j$是下面方程组的唯一解：
$$
\pi_j=\sum_{k=1}^{m}\pi_k p_{kj},j=1,...,m,
$$

$$
l=\sum_{k=1}^m\pi_k
$$

(c)另外有：
$$
\pi_j=0,对于所有的非常返状态j,
$$

$$
\pi_j>0,对于所有的常返态j.
$$

### 7.4 吸收概率和吸收的期望时间

一个常返态k是**吸收**的：$p_{kk}=1，p_{kj}=0$对于所有的$j\ne k$。

固定一个吸收态，设为s，令$a_i$表示链从状态i开始，最终达到s的概率即为吸收概率：
$$
a_i=P(X_n最终等于吸收状态s\mid X_0=i)
$$
**平均吸收时间方程组**：

平均吸收时间$\mu_1,...,\mu_m$是下列方程组的唯一解
$$
\mu_i=0,对于所有的非常返状态i，
$$

$$
\mu_i=1+\sum_{j=1}^m p_{ij}\mu_{j},对于所有的非常返状态i.
$$

**平均首访时间和回访时间方程组**：

考虑只有单个常返类的马尔可夫链，令s为特殊的常返状态。

- 从状态i到状态s的平均首访时间$t_i$，是下列方程组的唯一解
  $$
  t_s=0,t_i=1+\sum_{j=1}^m p_{ij}t_j，对于所有的i\ne s
  $$

- 状态s的平均回访时间$t^*_s$为

$$
t^*_s=1+\sum_{j=1}^m p_{sj}t_j
$$

### 7.5 连续时间的马尔可夫链

连续性时间马尔可夫链的假设

- 如果当前状态是i，到下一个转移的时间服从已知参数$v_i$的指数分布，且独立于之前的历史过程和下一个状态
- 如果当前状态是i，按照给定的概率$p_{ij}$到达下一个状态j,而且独立于之前的历史过程和转移到下一个状态的时间间隔



考虑一个具有单个常返类的连续时间马尔可夫链。那么，状态$j$和它对应的稳态概率$\pi_j$有如下性质。q是转移速率。

(a) 对于每个$j$，我们有：
$$
\lim_{t\to\infty}P(X(t)=j\mid X(0)=i)=\pi_j，对于所有的i
$$
(b) $\pi_j$是下面方程组的唯一解：
$$
\pi_j\sum_{k\ne j}q_{jk}=\sum_{k\ne j}^{m}\pi_k q_{kj},j=1,...,m,
$$

$$
l=\sum_{k=1}^m\pi_k
$$

(c)另外有：
$$
\pi_j=0,对于所有的非常返状态j,
$$

$$
\pi_j>0,对于所有的常返态j.
$$

  ## 第8章 贝叶斯统计推断

**统计推断**是从观测数据推断未知变量或未知模型的有关信息的过程，和概率理论不是一码事。

统计有两种学派：贝叶斯学派和经典（频率）学派。它们的重要区别在于如何看待未知模型或者变量。

- 贝叶斯学派将其看成已知分布的随机变量。
- 经典学派将其看成未知的待估计的量。

统计推断的应用主要分为两种类型：模型推断和变量推断。

统计推断问题可以简单分为估计问题和假设检验问题。在参数估计中对参数进行估计，使得在某种概率意义下估计接近真实值。在假设检验中，未知参数根据对应的假设可能取有限个值。人们去选择其中一个假设，目标是使犯错误的概率很小。

考虑具有形式$Y=g(X)+W$的模型，该模型涉及两个随机变量$X$和$Y$，其中$W$是零均值噪声，$g$是需要估计的未知函数。这类问题，未知目标（比如这里的函数g）是不能表述为固定数目的参数，称为**非参数**统计推断问题。不在此书考虑范围之内。

**共轭先验分布**：

在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而**先验分布被称为似然函数的共轭先验分布**。

具体地说，就是给定贝叶斯公式$p_{\Theta\mid X}(\theta\mid x)=\frac{p_\Theta(\theta)p_{X\mid\Theta}(x\mid \theta)}{\sum_{\theta'}p_{\Theta}(\theta')p_{X\mid\Theta}(x\mid\theta')}$（假设$\Theta$离散，$X$离散），假定似然函数$p_{X\mid\Theta}(x\mid \theta)$是已知的，问题就是选取什么样的先验分布会让后验分布与先验分布具有相同的数学形式。

共轭先验分布是针对某一参数而言，如正态分布关于方差的共轭先验分布为倒Gamma分布、关于均值的共轭先验分布为正态分布。

所有指数家族的分布都有共轭先验。

### 8.1 贝叶斯推断与后验分布

推断过程：

- 起点是未知随机变量$\Theta$的先验分布$p_{\Theta}$或者$f_\Theta$。
- 得到观测向量$X$的$p_{X\mid\Theta}$或者$f_{X\mid\Theta}$。
- 一旦$X$的一个特定值x观测到后，运用贝叶斯法则计算$\Theta$的后验分布。

贝叶斯法则的4种形式：

- $\Theta$离散，$X$离散：
  $$
  p_{\Theta\mid X}(\theta\mid x)=\frac{p_\Theta(\theta)p_{X\mid\Theta}(x\mid \theta)}{\sum_{\theta'}p_{\Theta}(\theta')p_{X\mid\Theta}(x\mid\theta')}
  $$
  
- $\Theta$离散，$X$连续：
  $$
  p_{\Theta\mid X}(\theta\mid x)=\frac{p_\Theta(\theta)f_{X\mid\Theta}(x\mid \theta)}{\sum_{\theta'}p_{\Theta}(\theta')f_{X\mid\Theta}(x\mid\theta')}
  $$

- $\Theta$连续，$X$离散：
  $$
  f_{\Theta\mid X}(\theta\mid x)=\frac{f_\Theta(\theta)p_{X\mid\Theta}(x\mid \theta)}{\sum_{\theta'}f_{\Theta}(\theta')p_{X\mid\Theta}(x\mid\theta')}
  $$
  
- $\Theta$连续，$X$连续：
  $$
  f_{\Theta\mid X}(\theta\mid x)=\frac{f_\Theta(\theta)f_{X\mid\Theta}(x\mid \theta)}{\sum_{\theta'}f_{\Theta}(\theta')f_{X\mid\Theta}(x\mid\theta')}
  $$
  



### 8.2 点估计，假设检验，最大后验概率准则

最大后验准则是指在所有的$\theta$中寻找$\hat{\theta}$，使得后验分布（见贝叶斯准则）达到最大值。

**点估计**指的是在得到实际观察值x的基础上我们选择的$\hat{\theta}$的数值。$\hat{\theta}$是由观测值x的某些函数g决定的，即$\hat{\theta}=g(x)$。随机变量$\hat{\Theta}=g(X)$也称为估计，之所以说$\hat{\Theta}$是随机变量是因为估计的结果由随机的观测值所决定。利用不同的函数g可以构造不同的估计量。

- **最大后验概率（MAP）估计量**。观测到x，在所有$\theta$中选$\hat{\theta}$使得后验分布达到最大。当取值很多时任取其中一个。

- **条件期望估计量**。$\hat{\theta}=E[\Theta\mid X=x]$

**假设检验**一般使用最大后验概率准则。

### 8.3 贝叶斯最小均方估计

条件期望估计量具有使可能的均方误差达到最小的性质。（最小均方简称为LMS）

### 8.4 贝叶斯线性最小均方估计

在一个较小的统计量的集合类中寻找统计量使得均方误差最小：那些观测值的线性函数的集合类。虽然这种统计量会导致较高的均方误差，但是在实际中有明显的优势：对计算要求简单，只包括均值、方差以及观测与参数之间的协方差。在最大后验估计量和最小均方估计量难以计算的情况下，这是个很有用的替代估计量。

## 第9章 经典统计推断

### 9.1 经典参数估计

经典的方法将参数$\theta$看作未知常数，而不是随机变量。

$\hat{\Theta}_n$是未知参数$\theta$的一个**估计量**，也即关于n个的观测$X_1,...,X_n$ (服从依赖参数$\theta$的分布)的一个函数。

- 估计误差，记为$\tilde{\Theta}_n$，定义为$\tilde{\Theta}_n=\hat{\Theta}_n-\theta$

- 估计量的偏差，记为$b_\theta(\hat{\Theta}_n)$，是估计误差的期望值：
  $$
  b_\theta(\hat{\Theta}_n)=E_\theta[\hat{\Theta}_n]-\theta
  $$
- $\hat{\Theta}_n$的期望值、方差和偏差都依赖于$\theta$，而估计误差同时还依赖于观测$X_1,...,X_n$ 
- 称$\hat{\Theta}_n$**无偏**，若$E_\theta[\hat{\Theta}_n]=\theta$对于$\theta$所有可能的取值都成立
- 称$\hat{\Theta}_n$**渐进无偏**，若$\lim_{n\to\infty}E_\theta[\hat{\Theta}_n]=\theta$对于$\theta$所有可能的取值都成立
- 称$\hat{\Theta}_n$为$\theta$的相合估计序列，如果对于参数所有可能的真值$\theta$，序列$\hat{\Theta}_n$依概率收敛到$\theta$

**最大似然估计**：

设观测向量$X=(X_1,...,X_n)$的联合分布列为$p_X(x;\theta)=p_X(x_1,....,x_n;\theta)$（$\theta$可为向量或数量），其中$x=(x_1,...x_n)$为$X$的观察值。**最大似然估计**是使（$\theta$的）数值函数$p_X(x_1,...,x_n;\theta)$达到最大的参数值：
$$
\hat{\theta}_n=\underset{\theta}{argmax} p_X(x_1,...,x_n;\theta)
$$
对于连续型随机变量：
$$
\hat{\theta}_n=\underset{\theta}{argmax} f_X(x_1,...,x_n;\theta)
$$
$p_X(x;\theta)$和$f_X(x;\theta)$称为似然函数。

**随机变量的均值和方差估计**：

观察值$X_1,...,X_n$是独立同分布的，均值$\theta$和方差$v$均未知。

- 样本均值
  $$
  M_n=\frac{X_1,...,X_n}{n}
  $$
  是$\theta$的一个无偏估计量，它的均方误差是$v/n$。

- 方差的估计量有两个
  $$
  \bar{S}^2_n=\frac{1}{n}\sum^n_{i=1}(X_i-M_n)^2,\hat{S}^2_n=\frac{1}{n-1}\sum^n_{i=1}(X_i-M_n)^2
  $$
  
- 当$X_i$服从正态分布，估计量$\bar{S}^2_n$和最大似然估计量相等。它有偏但是渐进无偏。估计量$\hat{S}^2_n$是无偏的。当n很大时，方差的两个估计量本质上一致。

**置信区间**：

- 对于一维的未知参数$\theta$，其置信区间是一个以很高概率包含$\theta$的区间，端点为$\hat{\Theta}_n^-$和$\hat{\Theta}_n^+$。

- $\hat{\Theta}_n^-$和$\hat{\Theta}_n^+$是依赖于观测值$X_1,...,X_n$的随机变量

- $(1-\alpha)$置信区间对于$\theta$所有可能的取值满足
  $$
  P(\hat{\Theta}_n^-\le\theta\le\hat{\Theta}_n^+)\ge1-\alpha
  $$

### 9.2 线性回归

  线性回归可以由最小二乘法完成操作，而不需要任何概率上的解释。它也可以在各种概率框架下解释。

### 9.3 简单假设检验

### 9.4 显著性检验

  

  
